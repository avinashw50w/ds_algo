# Problem

Imagine a web server for a simplified search engine. This system has 100 machines to respond to search queries, which may then call out using processSearch(string query) to another cluster of machines to actually get the result. The machine which responds to a given query is chosen at random, so you cannot guarantee that the same machine will always respond to the same request. The method processSearch() is very expensive. Design a caching mechanism for the most recent queries. Be sure to explain how you would update the cache when data changes.

## Open-ended discussion

As Phil Karlton says, there are only two hard problems in Computer Science: cache invalidation and naming things!

This problem would be much easier if the same query was always sent to the same machine: we could just build a caching layer on each machine, cache replies locally, and invalidate the cache whenever local information was updated. Unfortunately, this is not the case, so our system will have to be a little bit smarter than that.

Clearly, we need a caching layer manager that sits between the two clusters. This layer is a component in the architecture which acts as an intermediary between the processSearch() request and the cluster that actually processes the request. We can use the cluster's machines to create a distributed caching infrastructure (much like a distributed key-value store) in a similar way to that of memcached. Memcached uses a client-server model, where the client knows every server (this is reasonable since we're talking about a cluster of 100 machines), and uses that information to compute which server stores the value of a given key (if it's cached). Note that different machines in the cluster may be handed the results of a query that was answered by another machine; we essentially provide a deterministic hash function that no matter what, always maps a given query to the same machine. In other words, we build a deterministic system on top of the cluster, but just for the purposes of caching. This is a high-level view of how memcached works. The advantage of this architecture is that servers don't know each other, so we have a reasonably sharded and scalable shared-nothing module (no synchronization to deal with and no shared state, which is easier to manage, debug and maintain).

We do have a single point of failure, which is also a contention point - the caching layer manager. We could explore the possibility of eliminating the caching layer manager entirely and let the cluster that issues requests deal with the caching infrastructure, but then we would have to worry about synchronizing the caching state across the machines in that cluster, which is not really their job. Another approach is to make the caching layer smarter and distributed: for example, create an overlay network where each node keeps track of O(log(N)) nodes (for a cluster of size N), and requests are iteratively forwarded to the right machine (according to the hash value) in at most O(log(N)) steps (that is, use something like Chord Distributed Hash Table to associate query keys with the servers that cache the results). This is good because each client need not keep track of every other server anymore, and it also guarantees that a query, if cached, is deterministically placed on the same machine.

What about cache invalidation? Cache invalidation is a hard problem. The right approach here depends on where (and how) exactly the writes take place, and what kind of guarantees we want to provide to user code.

There are basically 3 ways to deal with data updates when caching is used:

* Write-through: in this paradigm, writes are performed on cache and then are passed through to the underlying storage medium. This eliminates any problems that might arise with cache inconsistency, but it comes at the cost of slower writes (assuming that the underlying storage is slow, as is usually the case). However, it has the added benefit of ensuring that writes are not lost if the server fails unexpectedly (of course, assuming that underlying storage is reliable). In other write policies, such as write-back, updated data may be lost if the machine fails unexpectedly.

* Write-back: write-back is fast; updates to the data are only written to cache (the data is only updated on disk when that specific cache entry is purged). It can have huge performance boosts, but it can be a problem if servers are expected to fail. This may not be a big deal if we're ok with missing a few minor updates to the search engine database, or it might be a huge deal if we want to offer stronger guarantees (for example, in a service like Dropbox it would be a catastrophe). If all we're looking for is efficiency and we're ok with some data loss / inconsistency, then this is definitely the way to go. For a search engine it *might* be a good choice, but not necessarily the best.

* Write-around: write-around takes a different approach and writes directly into the underlying medium, invalidating the cache entries affected (if there are any). This can be good for services where writes are more common than reads, because we don't keep useless data around in the cache that is not likely to be read. Yes, writes will be more expensive, but if we don't plan on reading back the data any time soon, then why bother? Just purge it and write directly on disk. For the purposes of a search engine, this doesn't look like a smart choice. Search engines have a *huge* read-to-write ratio, so it is very likely that updated data will be read very, very soon.

Taking into account the high read-to-write ratio, it might seem that write-back would be a good choice here. If we want to offer stronger consistency guarantees, we should look at write-through, or we can try some sort of cache-level replication by storing the same query with two different hashes that map to two different machines.
